# 1. ëª©í‘œ : ë‹¤ì¤‘ í´ë˜ìŠ¤ ë¶„ë¥˜ ì‹ ê²½ë§ êµ¬í˜„
# 2. ìš”êµ¬ì‚¬í•­
# 1) ì…ë ¥ì¸µ: 64ê°œ(8x8 ì´ë¯¸ì§€)
# 2) ì€ë‹‰ì¸µ: ì ì ˆí•œ í¬ê¸°
# 3) ì¶œë ¥ì¸µ: 10ê°œ(0-9 ìˆ«ì)
# 4) Softmax í™œì„±í™” í•¨ìˆ˜
# 5) Cross-Entropy ì†ì‹¤ í•¨ìˆ˜

from sklearn.datasets import load_digits
from sklearn.model_selection import train_test_split
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix
plt.rc('font', family='Apple SD Gothic Neo')
plt.rcParams['axes.unicode_minus'] = False

def mini_mnist():
    """
    8x8 ìˆ«ì ì´ë¯¸ì§€ ë¶„ë¥˜ (sklearn digits ë°ì´í„°ì…‹ ì‚¬ìš©)
    """
    # ë°ì´í„° ë¡œë“œ
    digits = load_digits()
    X, y = digits.data, digits.target
    X = X / 16.0 # ì •ê·œí™” (0-16 -> 0-1)

    # ì›-í•« ì¸ì½”ë”©
    y_onehot = np.eye(10)[y]
    num_classes = y_onehot.shape[1]

    # í›ˆë ¨/í…ŒìŠ¤íŠ¸ ë¶„í• 
    X_train, X_test, y_train, y_test = train_test_split(
        X, y_onehot, test_size=0.2, random_state=42
    )

    print("â“ ë¯¸ë‹ˆ MNIST ê³¼ì œ")
    print(f"í›ˆë ¨ ë°ì´í„° : {X_train.shape}")
    print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„° : {X_test.shape}")
    print("í´ë˜ìŠ¤ ìˆ˜ : 10ê°œ (0-9 ìˆ«ì)")

    def sigmoid(x):
        return 1 / (1 + np.exp(-x))

    def sigmoid_derivative(z):
        s = sigmoid(z)
        return s * (1 - s)
    
    class DigitClassifier:
        def __init__(self, input_size, hidden_size, output_size, lr=0.1):
            # ë„¤íŠ¸ì›Œí¬ êµ¬ì¡° ì„¤ê³„
            # input_size = 64, hidden_size = ì ì ˆí•˜ê²Œ ë°›ì•„ì˜¤ê¸°, output_size = 10
            self.W1 = np.random.randn(input_size, hidden_size) * 0.1
            print(f"\nW1 í˜•íƒœ: {self.W1.shape}")
            self.b1 = np.zeros((1, hidden_size))
            # print(f"b1 í˜•íƒœ: {self.b1.shape}")
            self.W2 = np.random.randn(hidden_size,output_size) * 0.1
            print(f"W2 í˜•íƒœ: {self.W2.shape}")
            self.b2 = np.zeros((1, output_size))
            # print(f"b2 í˜•íƒœ: {self.b2.shape}")
            self.lr = lr

            self.z1 = None
            self.a1 = None
            self.z2 = None
            self.a2 = None

        def softmax(self, z):
            # Softmax í•¨ìˆ˜ êµ¬í˜„
            # ë²¡í„°ë¥¼ ë°›ì•„ì„œ ê·¸ ê°’ì„ í™•ë¥  ë¶„í¬ë¡œ ë³€í™˜
            # ì¶œë ¥ì¸µì„ í™•ë¥ ë¡œ ë°”ê¿”ì£¼ëŠ” í•¨ìˆ˜, ë‹¤ì¤‘ í´ë˜ìŠ¤ ë¶„ë¥˜ì— í•„ìˆ˜
            z = z - np.max(z, axis=1, keepdims=True) # axis=1: ìˆ˜ì¹˜ì•ˆì •í™” / keepdims=True: (N,1)í˜•íƒœë¡œ ìœ ì§€
            exp_z = np.exp(z)
            return exp_z / np.sum(exp_z, axis=1, keepdims=True) # ê° í–‰ì˜ í•©ì´ 1ì´ ë˜ë„ë¡ ë‚˜ëˆ ì£¼ê¸°(í™•ë¥  ë¶„í¬)

        def cross_entropy_loss(self, y_pred, y_true):
            # Cross-Entropy ì†ì‹¤ êµ¬í˜„
            # -np.sum(y_true * np.log(y_pred)
            batch_size = y_pred.shape[0]
            return -np.sum(y_true * np.log(y_pred)) / batch_size
        
        def forward(self, X):
            # ìˆœì „íŒŒ
            self.z1 = np.dot(X, self.W1) + self.b1 # ì„ í˜• ë³€í™˜
            # print(f"1ë‹¨ê³„ - ì€ë‹‰ì¸µ ì…ë ¥ z1 í˜•íƒœ : {self.z1.shape}")
            self.a1 = sigmoid(self.z1) # í™œì„±í™” í•¨ìˆ˜
            # print(f"1ë‹¨ê³„ - ì€ë‹‰ì¸µ ì¶œë ¥ a1 í˜•íƒœ : {self.a1.shape}")
            self.z2 = np.dot(self.a1, self.W2) + self.b2
            # print(f"2ë‹¨ê³„ - ì¶œë ¥ì¸µ ì…ë ¥ z2 í˜•íƒœ : {self.z2.shape}")
            self.a2 = self.softmax(self.z2)
            # print(f"2ë‹¨ê³„ - ìµœì¢… ì¶œë ¥ a2 í˜•íƒœ : {self.a2.shape}")

            return self.a2

        def backward(self, X, y):
            # ì—­ì „íŒŒ
            N = X.shape[0]

            # ì¶œë ¥ì¸µ: softmax + cross-entropy -> dz2 = (y_pred - y_true)/N
            dz2 = (self.a2 - y) / N
            dW2 = np.dot(self.a1.T, dz2)
            db2 = np.sum(dz2, axis=0, keepdims=True)

            # ì€ë‹‰ì¸µ: a1 = sigmoid(z1)
            da1 = np.dot(dz2, self.W2.T)
            dz1 = da1 * sigmoid_derivative(self.z1)
            dW1 = np.dot(X.T, dz1)
            db1 = np.sum(dz1, axis=0, keepdims=True)

            self.W2 -= self.lr * dW2
            self.b2 -= self.lr * db2
            self.W1 -= self.lr * dW1
            self.b1 -= self.lr * db1

        def predict(self, X):
            # ì˜ˆì¸¡ í•¨ìˆ˜
            z1 = np.dot(X, self.W1) + self.b1
            a1 = sigmoid(z1)
            z2 = np.dot(a1, self.W2) + self.b2
            y_hat = self.softmax(z2)
            pred_idx = np.argmax(y_hat, axis=1)

            return pred_idx, y_hat

        def accuracy(self, X, y):
            # ì •í™•ë„ ê³„ì‚°
            pred_idx, _ = self.predict(X)
            true_idx = np.argmax(y, axis=1)

            return (pred_idx == true_idx).mean()

    model = DigitClassifier(input_size=64, hidden_size=256, output_size=num_classes, lr=0.1)
    epochs = 300
    
    losses = []
    for epoch in range(1, epochs + 1):
        output = model.forward(X_train)
        loss = model.cross_entropy_loss(output, y_train)
        model.backward(X_train, y_train)
        losses.append(float(loss))

        train_result = model.accuracy(X_train, y_train)
        test_result = model.accuracy(X_test, y_test)
        print(f"[{epoch:3d}/{epochs}] loss={loss:.4f} | train={train_result*100:.2f}% | test={test_result*100:.2f}%")

    print("\n ğŸŒŸ êµ¬í˜„ í›„ ë‹¤ìŒì„ í™•ì¸í•˜ì„¸ìš”:")
    print("- í›ˆë ¨ ì •í™•ë„: 90% ì´ìƒ")
    print("- í…ŒìŠ¤íŠ¸ ì •í™•ë„: 85% ì´ìƒ")
    print("- ê° ìˆ«ìë³„ ì˜ˆì¸¡ ì„±ëŠ¥")

    print(f"\nìµœì¢… ê²°ê³¼: train={train_result*100:.2f}% | test={test_result*100:.2f}%")

    y_true = np.argmax(y_test, axis=1)
    y_pred, _ = model.predict(X_test)
    cm = confusion_matrix(y_true, y_pred)
    per_class_accuracy = cm.diagonal() / cm.sum(axis=1)
    for i, acc in enumerate(per_class_accuracy):
        print(f"í´ë˜ìŠ¤ {i} ì •í™•ë„: {acc*100:.2f}%")

    if train_result >= 0.90 and test_result >= 0.85:
        print("\nì„±ê³µ!!!!!")
    else:
        print("\në‹¤ì‹œ í•´ë³´ìã…œã…œ~~~~")

    plt.figure(figsize=(12, 9))

    # ì†ì‹¤ ê·¸ë˜í”„
    plt.subplot(2, 2, 1)
    plt.plot(losses)
    plt.title('í•™ìŠµ ì†ì‹¤ ë³€í™” (Cross-Entropy)')
    plt.xlabel('ì—í¬í¬')
    plt.ylabel('ì†ì‹¤')
    plt.grid(True)

    # ê°€ì¤‘ì¹˜ ë³€í™” ì‹œê°í™”(ëœë¤ 200ê°œ)
    plt.subplot(2, 2, 2)
    # plt.bar(range(len(model.W1.flatten())), model.W1.flatten()) # ìƒ˜í”Œ ìˆ˜ê°€ ë„ˆë¬´ ë§ì•„ì„œ ê·¸ë˜í”„ê°€ ì œëŒ€ë¡œ í‘œí˜„ ì•ˆë¨
    idx = np.random.choice(model.W1.flatten().size, 200, replace=False) # ëœë¤ìœ¼ë¡œ 200ê°œì •ë„ ë½‘ì•„ì„œ í‘œí˜„
    plt.bar(range(200), model.W1.flatten()[idx])
    plt.title('ì€ë‹‰ì¸µ ê°€ì¤‘ì¹˜ (í•™ìŠµ í›„)')
    plt.xlabel('ê°€ì¤‘ì¹˜ ì¸ë±ìŠ¤')
    plt.ylabel('ê°€ì¤‘ì¹˜ ê°’')

    # ì˜ˆì¸¡ ê²°ê³¼ ë¹„êµ(ëœë¤ 1ê°œ) 
    plt.subplot(2, 2, 3)
    idx = np.random.randint(0, X_test.shape[0])
    _, prob_one = model.predict(X_test[idx:idx+1])
    true_one = y_test[idx]
    x_pos = np.arange(10)
    plt.bar(x_pos - 0.2, true_one.flatten(), 0.4, label='ì •ë‹µ(ì›-í•«)', alpha=0.7)
    plt.bar(x_pos + 0.2, prob_one.flatten(), 0.4, label='ì˜ˆì¸¡ í™•ë¥ ', alpha=0.7) 
    plt.title(f'ì˜ˆì¸¡ ê²°ê³¼ ë¹„êµ (ìƒ˜í”Œ idx={idx}, ì‹¤ì œ={np.argmax(true_one)}, ì˜ˆì¸¡={np.argmax(prob_one)})')
    plt.xlabel('í´ë˜ìŠ¤(0~9)')
    plt.ylabel('ê°’')
    plt.legend()
    plt.xticks(x_pos, list(range(10)))
    
    # ì‹ ê²½ë§ êµ¬ì¡° ì‹œê°í™”
    plt.subplot(2, 2, 4)
    plt.text(0.15, 0.7, 'ì…ë ¥ì¸µ\n(64ê°œ)', ha='center', va='center',
             bbox=dict(boxstyle="round", facecolor='lightblue'))
    plt.text(0.50, 0.7, f'ì€ë‹‰ì¸µ\n({model.W1.shape[1]}ê°œ)', ha='center', va='center',
             bbox=dict(boxstyle="round", facecolor='lightgreen'))
    plt.text(0.85, 0.7, 'ì¶œë ¥ì¸µ\n(10ê°œ)', ha='center', va='center',
             bbox=dict(boxstyle="round", facecolor='lightcoral'))

    # í™”ì‚´í‘œ
    plt.arrow(0.22, 0.70, 0.20, 0.0, head_width=0.02, head_length=0.02, fc='black', length_includes_head=True)
    plt.arrow(0.57, 0.70, 0.20, 0.0, head_width=0.02, head_length=0.02, fc='black', length_includes_head=True)

    plt.xlim(0, 1)
    plt.ylim(0.5, 0.9)
    plt.title('ì‹ ê²½ë§ êµ¬ì¡°')
    plt.axis('off')

    plt.tight_layout()
    plt.show()

mini_mnist()